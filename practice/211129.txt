회귀&분류
회귀 : 선과 점과 거리
분류 : 이진분류 / 다중분류

0과 1의 이진분류

지표를 mse로 선과의 간격을 판단했지만 오늘은 vinary~를 쓰겠다.

┌		★★sigmoid★★			┐

Sigmoid는 대표적인 Logistic 함수입니다.
Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 
.... sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.

Sigmoid 함수는 binary classification 에 적절함 함수다.
일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식이다.
딥러닝에서는 특정 임계치를 넘을 때만 활성화되기 때문에 activation function 중의 하나로 구분되는 함수다.
└						┘

Activation 활성함수 사용
y=wx1+w2x2+w3x3+b
딥러닝에서 부동소수점 연산이 가장 좋다.
default로 Linear라는 활성함수를 항상 사용하고 있었다.
linear(y=wx+b)를 항상 전하고 있었다.

signoid(x)/y = 1/ 1+e^-x 라면 x가 커지면 e가 0 으로가니까 1, 작아지면 분모가 커지니까 0으로 가까워진다
so, 값 자체를 0과 1사이로 한정시키고 싶다? signoid !

0인지 1인지 판단하고 싶은 모델을 만들고 싶다면 signoid를 하면 되겠다.
히든레이어는 한정시켜도 되겠다.
중간부분은 학습을 시키는부분

0.5기준 이상= 1판단, 이하=0판단 이런식..?!
左右
loss = mse = binary cross entropy
100% 이진분류는 binary cross entropy
★ 이진분류의 액티베이션은 시그노이드
★ 이진분류의 로스는 바이너리 크로스 엔트로피

회귀모델에서의 마지막 activation !? 리니어 였겠지

분류모델에서는 accuracy를 가저다 매드릭에서 쓰면되는데.

evaluate의 첫번째는 항상 고정, 전체 loss값


모델을 구성하여 데이터를 훈련시키고 잘 훈련된 모델인지 검증하기 위한 척도로 loss와 acc를 사용합니다.

 
값이 0이나 1이여야 쓸수있는거지
수수점같은 회귀모델은 돌릴수 없어 돌려야지겠지 쓰레기가 될뿐

데이터에 관하여...
정제된 데이터 - 값이 너무 한쪽으로 치우쳐있으면 accuracy 값이 잘 안 맞을 수 있다.
ex) 암측정병원에서 병원에 온 암발병자가 0~1명인데 본인들의 암측정률은 99.99퍼다 라고 하는격




트레인/테스트를 나누지 않고 데이터로 전부 실험 진행하면 
데이터의 특성에 너무 잘 맞춰서 모델링이 되어버리고 데이터에 과적합되어버려서 곡선이 너무 강해지는 경향이있다
최대한 단순화된 선으로 아웃라이어 등을 쳐낸 모델은 오히려 좋은 성능을 낼 수도 있다.
그렇기에 선은 가급적이면 직선형이 나을 것이다(데이터가 과적합 안되게 트레인 테스트를 잘 하는)

sigmoid의 목적 편하게 0&1로 잡으려는건데
output이 3개라면?? activation에 문제가 있네
Activation을 linear로 잡으면 또 원래 값나오는데
새로운 Activation인 softmax가 등장한다,!
쌍으로 loss도 바꿔줘야겠지?! binary아닌 Categorical_CrossEntropy를 쓴다.
다중분류는 마지막 최종 activation은 softmax로.
loss는 categorical_crossentropy를 쓴다. metrics도 accuracy로 동일하게 쓴다

소프트맥스 함수(Softmax Function)
소프트 맥스는 세 개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수다.
소프트 맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.

대충 막 필기중

손실=loss=cost

이진분류 - 영역을 긋는 개념

활성화 함수 - 이진분류 - 시그노이드 - 로스 :binary_crossentropy

다중분류 문제네?? 평가지표-loss(categorical crossentropy)
softmax 라벨값이 3개라면 노드개수는 총합이 1이되는

시그노이드는 0~1사이인데 소프트맥스는 총합이 1이.
소프트맥스는 중간레이어를 쓰지 않는다.
△ 0.43 0.43이 가장 좋은 거겠다.
○ 0.27
□ 0.3
이 세개의 값을 outlayer 노드 하나에 다 넣을 수 있을까? 없겠다. 노드도 3개가 필요한 것이다.!
즉 최종노드는 n개여야 한다( n개의 총합은 1이됨(ex) 0.43+0.27+0.3) categorical crossentropy로 가장 큰 놈으로 판단
다중분류에선 마지막 activation softmax
loss는 categorical c,e를 쓴다
마지막 라벨 loss의 개수는 우리가 분류할 라벨 n개와 같이 n개이다.
컬럼 10개면 inputnode도 10개겠다.

		q)))))))))))))))))))))none,3 열중요 행무시 이유


input (N,1) n개 스칼라를 가진 벡터 1개
	O   O   O   O
	 O   O   O	ㅣ
	O O O O O O  히든레이어
	   O    O		ㅣ
	O   O   O 
output (N,3) n개 스칼라를 가진 벡터 3개

▼
input y와 output y가 같나? = 비교가 안됨 규격이 다르니까.

ㅣxxx△△△
ㅣo△ooxx
ㅣxxoxo△
ㅣㅡㅡㅡㅡㅡㅡ
0X 1 세모 2 네모
숫자 높은놈찾지? y=wx+b에 ㅁ 들어갈땐2 세모들어갈땐1
x는 가치가 없고 네모는 세모의 두배
결국 가치는 큰놈한테 영향을 많이 받을 거다.


softmax자체가 큰놈찾는거니 큰쪽을 가르킨다.
2는 1의 두배라는 걸 인식하면 안되고 0 1 2 는 다 공평해야하고
얘 자체는 절대 2가 1의 두배가 되면 안된다.
→→→→★★★그렇기에 OneHotEncoding를 찾는다
2를 넣은값이 큰값이 나온다는 것은 2에 편향된다는 말이다.
0 1 2 는 단순 라벨값을 지정한 것이다. 번호로 0번,1번,2번 이렇게 정해줬다면 이건 가치가 아니라 그냥 라벨이다.
cat0 dog1 dragon2 로 라벨링했다
x컬럼이 4개= input4개로 셋하고 중간레이어를 만들었고  y 아웃풋은 하나이다.(전형적인 회귀모델)

라벨값을 세개를 쓴다면 세개의 노드값을 가질거다
x=(1000,4)
1000개의 스칼라가 가진 1개의 벡터이다
softmax를 통과했다면? x그대로 (1000,4) y는 (1000,3)

0 ㅣ 1 + 0 + 0 + 0
ㅡㅡㅡㅡㅡㅡㅡㅡㅡ
1 ㅣ 0 + 1 + 0 + 0
ㅡㅡㅡㅡㅡㅡㅡㅡㅡ
2 ㅣ 0 + 0 + 1 + 0
ㅡㅡㅡㅡㅡㅡㅡㅡㅡ
3 ㅣ 0 + 0 + 0 + 1
ㅡㅡㅡㅡㅡㅡㅡㅡㅡ
input data를 변환시켜버림
다중분류는 X=(1000,4)(N,4)에서
y값을(x는 건드리지 않아) 원핫인코딩해준다 아까와 같은 방식으로.
원핫인코딩을 통과한다면 (N, 3)이 되는 거고
y역시 y=(N,1)에서 (N,3)이 되어

다중분류 하는법)
이진분류할건지 다준분류할건지 봐서
다중분류라고 판단되면 0123 몇개있는지 확인하고
다중분류의 라벨이 몇갠지 확인해서
3개,4개 5개...so on하다면
x값을 라벨값에 맞추어 원핫인코딩해주어야한다
이렇게 shape이 갖춰진다면 softmax를 해준다
그리하여 통과한 y의 총합은 1이다
0.5 0.4 0.1이였는데 0.4 0.5 0.1이라면 accyracy가 0.1이 차이나겠지? 이런거 확인

모델 만들때 통상적으로 만들고 마지막은 라벨의 개수만큼 아웃풋이 되고 액티베이션을 소프트 맥스로 한정해준다
그러고 나서 로스값을 카테고리컬 크로스 엔트로피를 써준다.
정리) 액티베이션은 소프트맥스, loss는 카테고리컬, x는 반드시 원핫엔코딩해줘야 하겠다!


라벨링을 통해 0일경우 확률 1일경우 확률 2일경우 확률
숫자로서의 0 1 2 가 되면 값없음, 1, 1의2배 이런식으로 되어버려 가중치가 커지니까.



batch_size: 정수 혹은 None. 경사 업데이트 별 샘플의 수. 따로 정하지 않으면 batch_size는 디폴트 값인 32가 됩니다.
예를들어 batch_size를 1로설정하고 돌리면 1epoch당 본 소스에서는 1epoch당 batchsize가 371847이다
다시, fit부분에서 batch_size를 제거하고 돌려보면 11621이 나온다.
371847을 11621로 나눠보면 31.9978...으로 batch_size의 default값은 약 32인 것을 확인할 수 있다.